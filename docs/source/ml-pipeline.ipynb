{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Machine Learning Pipeline\n",
    "\n",
    "** WORK IN PROGRESS ** This guide isn't complete but the code examples may be useful as is.\n",
    "\n",
    "This guide assumes you are familiar with all the content in the [Introductory Guide](intro-guide.ipynb).\n",
    "\n",
    "A typical machine learning pipeline consists of loading data, extracting features, training models and storing the models for later use in a production system or further analysis. In some cases the feature extraction process is quick and the features are transitory without any need of saving them independently of the finished trained model. Other times the features are a representation of the data that you wish to reuse in different settings, e.g. in a dashboard explaining predictions, ad-hoc analysis, further model development. \n",
    "\n",
    "In the end a good deal of plumbing is required to wire up an app/service with the latest models and features in such a way that API calls can be traced back to the originating model, features, and even data sources. `provenance` abstracts much of this plumbing so you can focus on writing parsimonious pythonic pipelines&trade; with plain old functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            require(\n",
       "                [\n",
       "                    \"notebook/js/codecell\",\n",
       "                    \"codemirror/mode/yaml/yaml\"\n",
       "                ],\n",
       "                function(cc){\n",
       "                    cc.CodeCell.options_default.highlight_modes.magic_yaml = {\n",
       "                        reg: [\"^%%yaml\"]\n",
       "                    }\n",
       "                }\n",
       "            );\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%yaml basic_config\n",
    "blobstores:\n",
    "    disk:\n",
    "        type: disk\n",
    "        cachedir: /tmp/provenance-ml-artifacts\n",
    "        read: True\n",
    "        write: True\n",
    "        delete: True\n",
    "artifact_repos:\n",
    "    local:\n",
    "        type: postgres\n",
    "        db: postgresql://localhost/provenance-ml-guide\n",
    "        store: 'disk'\n",
    "        read: True\n",
    "        write: True\n",
    "        delete: True\n",
    "        # this option will create the database if it doesn't exist\n",
    "        create_db: True\n",
    "default_repo: local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running stamp_revision  -> e0317ab07ba4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<provenance.repos.Config at 0x11200ebe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import provenance as p\n",
    "\n",
    "\n",
    "p.load_config(basic_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.utils import check_random_state\n",
    "import toolz as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@p.provenance()\n",
    "def load_data(query):\n",
    "    # fetch something from the DB in real life...\n",
    "    random_state = check_random_state(abs(hash(query)) // (10**10))\n",
    "    return random_state.uniform(0, 10, 10)\n",
    "\n",
    "\n",
    "@p.provenance()\n",
    "def extract_features_a(data, hyperparam_a=5):\n",
    "    time.sleep(2)\n",
    "    rs = check_random_state(hyperparam_a)\n",
    "    return data[0:5] + 1 + rs.rand(5)\n",
    "\n",
    "\n",
    "@p.provenance()\n",
    "def extract_features_b(data, hyperparam_x=10):\n",
    "    time.sleep(2)\n",
    "    rs = check_random_state(hyperparam_x)\n",
    "    return data[5:] + 1 + rs.rand(5)\n",
    "\n",
    "\n",
    "@p.provenance()\n",
    "def build_model(features_a, features_b, num_trees=100):\n",
    "    return {'whatever': 'special model with {} trees'.format(num_trees)}\n",
    "\n",
    "\n",
    "@p.provenance()\n",
    "def evaluate(model, data):\n",
    "    return {'some_metric': 0.5, 'another_metric': 0.4}\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(train_query='some query', valid_query=\"another query\", hyperparam_a=5, hyperparam_x=10):\n",
    "    data = load_data(\"some query\")\n",
    "    features_a = extract_features_a(data, hyperparam_a)\n",
    "    features_b = extract_features_b(data, hyperparam_x)\n",
    "    model = build_model(data, features_a, features_b)\n",
    "\n",
    "    validation_data = load_data(\"another query\")\n",
    "    evaluation = evaluate(model, validation_data)\n",
    "\n",
    "    return {'features_a': features_a, 'features_b': features_b,\n",
    "            'model': model, 'evaluation': evaluation}\n",
    "\n",
    "@p.provenance()\n",
    "def make_decision(model, request):\n",
    "    # make some sort of prediction, classification, with the model\n",
    "    # to help make a 'decision' and return it as the result\n",
    "    return {'prediction': 0.5, 'model': model.artifact.id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** explain everything.. including the concept of artifact sets and how they simpify the building and deployment of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_production_pipeline():\n",
    "    with p.capture_set('production'):\n",
    "        return pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "res = run_production_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "res = p.get_set_by_name('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArtifactSet(id='08f3c7c6a84132faa155ca9996a26c4df92bd798', artifact_ids=frozenset({'2411521185b4267706a24f85b16c46e3a24b4e66', '96c47ddbeff008e2b3a27913611c9648c3e74aa2', 'd3bb8e7625b7093b079bdc8b7d50c6eaaa62f835', '46268ac8c40932b63033b387aa0217974c82c717', 'd3c930d243d6ec4d7be481ddd1f4c3e9277d5f09', '3fdafd792f113c669d55b416bed9b5091f954029'}), created_at=datetime.datetime(2017, 5, 1, 0, 1, 9, 119196), name='production')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "build_artifacts = res.proxy_dict(group_artifacts_of_same_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__main__.load_data', '__main__.build_model', '__main__.extract_features_b', '__main__.evaluate', '__main__.extract_features_a'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_artifacts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_artifacts['__main__.build_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<provenance.ArtifactProxy(46268ac8c40932b63033b387aa0217974c82c717) {'whatever': 'special model with [ 9.01053908  9.49144101  2.69614552  5.28085722  3.44221989] trees'} >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:provenance-dev]",
   "language": "python",
   "name": "conda-env-provenance-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "name": "Introduction Guide.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
